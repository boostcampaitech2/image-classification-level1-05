{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb3682bf-e5f3-4992-89b0-5e672c73a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4455a2f7-081d-4845-98b5-9b5cc35cadde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskDataset(Dataset):\n",
    "    def __init__(self, data_path, pre_transform, transform):\n",
    "        ######################################TODO######################################\n",
    "        df = pd.read_csv(data_path + 'train_with_label.csv')\n",
    "        self.X = []\n",
    "        for x in df['path']:\n",
    "            self.X.append(pre_transform(Image.open(x)))\n",
    "        self.y = df['label']\n",
    "        \n",
    "        self.classes = ['0 - w/m/<30', '1 - w/m/<60', '2 - w/m/>60', '3 - w/f/<30', '4 - w/f/<60', '5 - w/f/>60', '6 - i/m/<30', '7 - i/m/<60', '8 - i/m/>60', \n",
    "                        '9 - i/f/<30', '10 - i/f/<60', '11 - i/f/>60', '12 - n/m/<30', '13 - n/m/<60', '14 - n/m/>60', '15 - n/f/<30', '16 - n/f/<60', '17 - n/f/>60']\n",
    "        self.transform = transform   \n",
    "        ################################################################################\n",
    "\n",
    "    def __len__(self):\n",
    "        len_dataset = None\n",
    "        ######################################TODO######################################\n",
    "        len_dataset = len(self.X)\n",
    "        ################################################################################\n",
    "        return len_dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X,y = None, None\n",
    "        ######################################TODO######################################\n",
    "        X, y = self.transform(self.X[idx]), self.y[idx]\n",
    "        ################################################################################\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8645bf3b-bddc-4e12-aa66-6b1a834b70b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18900"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MaskDataset(\n",
    "    data_path = \"\",\n",
    "    pre_transform = transforms.Compose([\n",
    "        transforms.Resize((512//3,384//3)),\n",
    "        transforms.CenterCrop((64, 64)),\n",
    "    ]),\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7169eb94-f0ef-4d9b-a73b-efc8de75c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train_mask = DataLoader(\n",
    "    dataset,\n",
    "    batch_size = 64,\n",
    "    shuffle = True,\n",
    "    num_workers = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0303879b-6fea-4b1c-a458-b10a3f3cb13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(dataloader_train_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1e4009d-a0e8-46f0-a55b-73d4e0189cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e174fdf-d008-4f7c-b243-8192497f4026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0247,  0.0165, -0.0366, -0.0029,  0.0064,  0.0301, -0.0437, -0.0148,\n",
       "        -0.0078,  0.0339, -0.0150,  0.0021, -0.0434, -0.0065,  0.0301, -0.0076,\n",
       "        -0.0355,  0.0225])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASS_NUM = 18\n",
    "model.fc = torch.nn.Linear(in_features=512, out_features=CLASS_NUM, bias=True)\n",
    "torch.nn.init.xavier_uniform_(model.fc.weight)\n",
    "stdv = 1. / math.sqrt(model.fc.weight.size(1))\n",
    "model.fc.bias.data.uniform_(-stdv, stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c76de4c8-486b-475b-9baf-17745196e7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): BasicBlock(\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (1): BasicBlock(\n",
       "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layer1.requires_grad_(False)\n",
    "model.layer2.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f238a478-7c56-46ed-9864-c5d73d993e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파라미터 conv1.weight         가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 bn1.weight           가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 bn1.bias             가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer1.0.conv1.weight 가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer1.0.bn1.weight  가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer1.0.bn1.bias    가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer1.0.conv2.weight 가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer1.0.bn2.weight  가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer1.0.bn2.bias    가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer1.1.conv1.weight 가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer1.1.bn1.weight  가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer1.1.bn1.bias    가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer1.1.conv2.weight 가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer1.1.bn2.weight  가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer1.1.bn2.bias    가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer2.0.conv1.weight 가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer2.0.bn1.weight  가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer2.0.bn1.bias    가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer2.0.conv2.weight 가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer2.0.bn2.weight  가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer2.0.bn2.bias    가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer2.0.downsample.0.weight 가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer2.0.downsample.1.weight 가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer2.0.downsample.1.bias 가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer2.1.conv1.weight 가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer2.1.bn1.weight  가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer2.1.bn1.bias    가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer2.1.conv2.weight 가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer2.1.bn2.weight  가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer2.1.bn2.bias    가 gradient 를 tracking 하나요? -> False\n",
      "파라미터 layer3.0.conv1.weight 가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer3.0.bn1.weight  가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer3.0.bn1.bias    가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer3.0.conv2.weight 가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer3.0.bn2.weight  가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer3.0.bn2.bias    가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer3.0.downsample.0.weight 가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer3.0.downsample.1.weight 가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer3.0.downsample.1.bias 가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer3.1.conv1.weight 가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer3.1.bn1.weight  가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer3.1.bn1.bias    가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer3.1.conv2.weight 가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer3.1.bn2.weight  가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer3.1.bn2.bias    가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer4.0.conv1.weight 가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer4.0.bn1.weight  가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer4.0.bn1.bias    가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer4.0.conv2.weight 가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer4.0.bn2.weight  가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer4.0.bn2.bias    가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer4.0.downsample.0.weight 가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer4.0.downsample.1.weight 가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer4.0.downsample.1.bias 가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer4.1.conv1.weight 가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer4.1.bn1.weight  가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer4.1.bn1.bias    가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer4.1.conv2.weight 가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer4.1.bn2.weight  가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 layer4.1.bn2.bias    가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 fc.weight            가 gradient 를 tracking 하나요? -> True\n",
      "파라미터 fc.bias              가 gradient 를 tracking 하나요? -> True\n"
     ]
    }
   ],
   "source": [
    "for param, weight in model.named_parameters():\n",
    "    print(f\"파라미터 {param:20} 가 gradient 를 tracking 하나요? -> {weight.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aeafca11-3cab-4a8f-accd-86358f412c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 is using!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # 학습 때 GPU 사용여부 결정. Colab에서는 \"런타임\"->\"런타임 유형 변경\"에서 \"GPU\"를 선택할 수 있음\n",
    "\n",
    "print(f\"{device} is using!\")\n",
    "\n",
    "model.to(device) # Resnent 18 네트워크의 Tensor들을 GPU에 올릴지 Memory에 올릴지 결정함\n",
    "\n",
    "LEARNING_RATE = 0.0001 # 학습 때 사용하는 optimizer의 학습률 옵션 설정\n",
    "NUM_EPOCH = 10 # 학습 때 mnist train 데이터 셋을 얼마나 많이 학습할지 결정하는 옵션\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss() # 분류 학습 때 많이 사용되는 Cross entropy loss를 objective function으로 사용 - https://en.wikipedia.org/wiki/Cross_entropy\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE) # weight 업데이트를 위한 optimizer를 Adam으로 사용함\n",
    "\n",
    "dataloaders = dataloader_train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dde3fe03-d84d-49cf-bd89-df2e6f9d409f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b439754524a34abea4473272cc648440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=296.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-0의 데이터 셋에서 평균 Loss : 0.264, 평균 Accuracy : 0.904, 평균 f1: 0.850\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e2ef7052274fd5bfe06f6e4c8a8740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=296.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-1의 데이터 셋에서 평균 Loss : 0.179, 평균 Accuracy : 0.936, 평균 f1: 0.893\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6a3f10b1104a66834b506b81f67162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=296.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-2의 데이터 셋에서 평균 Loss : 0.141, 평균 Accuracy : 0.950, 평균 f1: 0.914\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29e03c0b3664e439f7c51ffc74a6172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=296.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-3의 데이터 셋에서 평균 Loss : 0.105, 평균 Accuracy : 0.964, 평균 f1: 0.938\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb1855573664b66a9a1ad91c44d6a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=296.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-4의 데이터 셋에서 평균 Loss : 0.094, 평균 Accuracy : 0.967, 평균 f1: 0.941\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13022db650d47caa3c11e459066b5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=296.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-5의 데이터 셋에서 평균 Loss : 0.085, 평균 Accuracy : 0.971, 평균 f1: 0.954\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cfbe0baea44f6d9c08cf62957fd5b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=296.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-6의 데이터 셋에서 평균 Loss : 0.071, 평균 Accuracy : 0.977, 평균 f1: 0.958\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3379fd56edc455bbef808934f6264c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=296.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-7의 데이터 셋에서 평균 Loss : 0.065, 평균 Accuracy : 0.977, 평균 f1: 0.965\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce73f878c91474aa366a9374c649b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=296.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-8의 데이터 셋에서 평균 Loss : 0.061, 평균 Accuracy : 0.980, 평균 f1: 0.969\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43321c802d2f4edba6bc9de4eafef792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=296.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-9의 데이터 셋에서 평균 Loss : 0.051, 평균 Accuracy : 0.982, 평균 f1: 0.972\n",
      "학습 종료!\n",
      "최고 accuracy : 0.9819576740264893, 최고 낮은 loss : 0.05141256064255401\n"
     ]
    }
   ],
   "source": [
    "### 학습 코드 시작\n",
    "from tqdm.notebook import tqdm \n",
    "best_test_accuracy = 0.\n",
    "best_f1 = 0\n",
    "best_test_loss = 9999.\n",
    "train_loss =[]\n",
    "test_loss = []\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    n_iter = 0\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    running_f1 = 0\n",
    "\n",
    "    model.train() # 네트워크 모델을 train 모드로 두어 gradient을 계산하고, 여러 sub module (배치 정규화, 드롭아웃 등)이 train mode로 작동할 수 있도록 함\n",
    "    for ind, (images, labels) in enumerate(tqdm(dataloaders)):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() # parameter gradient를 업데이트 전 초기화함\n",
    "\n",
    "        with torch.set_grad_enabled(True): # train 모드일 시에는 gradient를 계산하고, 아닐 때는 gradient를 계산하지 않아 연산량 최소화\n",
    "            logits = model(images)\n",
    "            _, preds = torch.max(logits, 1) # 모델에서 linear 값으로 나오는 예측 값 ([0.9,1.2, 3.2,0.1,-0.1,...])을 최대 output index를 찾아 예측 레이블([2])로 변경함  \n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            loss.backward() # 모델의 예측 값과 실제 값의 CrossEntropy 차이를 통해 gradient 계산\n",
    "            optimizer.step() # 계산된 gradient를 가지고 모델 업데이트\n",
    "\n",
    "        running_loss += loss.item() * images.size(0) # 한 Batch에서의 loss 값 저장\n",
    "        running_acc += torch.sum(preds == labels.data) # 한 Batch에서의 Accuracy 값 저장\n",
    "        running_f1 += f1_score(preds.cpu().numpy(), labels.cpu().numpy(), average='macro')\n",
    "        n_iter += 1\n",
    "\n",
    "    # 한 epoch이 모두 종료되었을 때,\n",
    "    epoch_loss = running_loss / len(dataloaders.dataset)\n",
    "    epoch_acc = running_acc / len(dataloaders.dataset)\n",
    "    epoch_f1 = running_f1 / n_iter\n",
    "\n",
    "    train_loss.append(epoch_loss)\n",
    "    print(f\"현재 epoch-{epoch}의 데이터 셋에서 평균 Loss : {epoch_loss:.3f}, 평균 Accuracy : {epoch_acc:.3f}, 평균 f1: {epoch_f1:.3f}\")\n",
    "    if best_f1 < epoch_f1: # phase가 test일 때, best accuracy 계산\n",
    "        best_f1 = epoch_f1\n",
    "        torch.save(model, 'bestmodel.pth')\n",
    "    if best_test_accuracy < epoch_acc: # phase가 test일 때, best accuracy 계산\n",
    "        best_test_accuracy = epoch_acc\n",
    "    if best_test_loss > epoch_loss: # phase가 test일 때, best loss 계산\n",
    "        best_test_loss = epoch_loss\n",
    "        \n",
    "print(\"학습 종료!\")\n",
    "print(f\"최고 accuracy : {best_test_accuracy}, 최고 낮은 loss : {best_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d0fc2c3-b6cf-4e75-be68-54290eb2f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '/opt/ml/input/data/eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9813a230-f1e9-4432-8696-fc6cbab20d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "526ea434-5d2e-483d-9307-37a3463cbfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512//3,384//3)),\n",
    "    transforms.CenterCrop((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "model = torch.load('bestmodel.pth')\n",
    "#model = MyModel(num_classes=18).to(device)\n",
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'resnet18_no_pretrain.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d216edbf-d955-41eb-be8d-21b3f1418d30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
